# backend/services/order_flow/manager.py
import logging
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from ...models import OrderFlowSyncState
from .fetcher import OrderFlowFetcher
from .processor import OrderFlowProcessor
from .storage import OrderFlowService

logger = logging.getLogger("OrderFlowManager")

# è‡ªåŠ¨åŒæ­¥åŒºåŸŸ
AUTO_AREAS = ["SE3"]
INITIAL_START_DATE = "2025-01-01T00:00:00"

class OrderFlowManager:
    def __init__(self, db: Session):
        self.db = db
        self.fetcher = OrderFlowFetcher()
        self.processor = OrderFlowProcessor()
        self.storage = OrderFlowService(db)

    def _get_or_create_state(self, area: str) -> OrderFlowSyncState:
        state = self.db.query(OrderFlowSyncState).filter(OrderFlowSyncState.area == area).first()
        if not state:
            start_time = datetime.fromisoformat(INITIAL_START_DATE)
            # å®æ—¶æŒ‡é’ˆé»˜è®¤å›æº¯1å°æ—¶
            realtime_start = datetime.utcnow() - timedelta(hours=1)
            
            state = OrderFlowSyncState(
                area=area, 
                last_archived_time=start_time,
                last_realtime_time=realtime_start 
            )
            self.db.add(state)
            self.db.commit()
            logger.info(f"[{area}] åˆå§‹åŒ–åŒæ­¥çŠ¶æ€: é»˜è®¤å›æº¯1å°æ—¶")
        return state

    def _update_state(self, state, **kwargs):
        for k, v in kwargs.items():
            setattr(state, k, v)
        state.updated_at = datetime.utcnow()
        self.db.commit()

    def sync_history_backfill(self, area: str):
        """
        ã€å†å²å½’æ¡£ã€‘
        ç­–ç•¥ï¼šç”±äº OrderBook/ByContractId æœ‰ 14-38 å°æ—¶å»¶è¿Ÿï¼Œ
        æˆ‘ä»¬åªå½’æ¡£ '48å°æ—¶å‰' çš„æ•°æ®ï¼Œç¡®ä¿æ•°æ®å·²å°±ç»ªã€‚
        """
        state = self._get_or_create_state(area)
        
        # å½’æ¡£çº¿ï¼šè®¾ç½®ä¸º 48 å°æ—¶å‰ (é¿å¼€ 38h çš„æœ€å¤§å»¶è¿Ÿ)
        archive_limit = datetime.utcnow() - timedelta(hours=48)
        
        curr = state.last_archived_time
        if curr >= archive_limit:
            return

        logger.info(f"[{area}] ğŸ“š å¯åŠ¨å†å²å½’æ¡£ (API A): {curr} -> {archive_limit}")
        
        while curr < archive_limit:
            # æ¯æ¬¡å¤„ç†ä¸€å¤©
            day_start = curr
            day_end = curr + timedelta(days=1)
            
            # ä¸ºäº†é˜²æ­¢æ­»å¾ªç¯ï¼Œè‹¥ day_end è¶…è¿‡ limit åˆ™æˆªæ–­
            if day_end > archive_limit:
                break
            
            target_date_str = day_start.strftime('%Y-%m-%d')
            
            try:
                # 1. è·å–è¯¥æ—¥æœŸçš„åˆçº¦åˆ—è¡¨
                # API: OrderBook/ContractsIds/ByArea
                contract_resp = self.fetcher.fetch_contract_list(area, day_start, day_end)
                contracts = contract_resp.get('contracts', [])
                
                if contracts:
                    logger.info(f"[{area}] {target_date_str} å…±æœ‰ {len(contracts)} ä¸ªåˆçº¦éœ€å½’æ¡£")
                    
                    for c in contracts:
                        cid = c.get('contractId') or c.get('id')
                        
                        # 2. æŠ“å–è¯¥åˆçº¦çš„å®Œæ•´å†å² (å« Snapshots)
                        # API: OrderBook/ByContractId
                        book_data = self.fetcher.fetch_historical_book(cid)
                        
                        # 3. æ„é€ å…ƒæ•°æ® (API A å¯èƒ½ä¸è¿”å› delivery areaï¼Œéœ€æ‰‹åŠ¨è¡¥å…¨)
                        meta = {
                            'contract_name': c.get('contractName'),
                            'delivery_start': c.get('deliveryStart'),
                            'delivery_end': c.get('deliveryEnd'),
                            'delivery_area': area
                        }
                        
                        # 4. è§£æå¹¶å…¥åº“ (é‡ç‚¹æ˜¯ Snapshots)
                        snapshots = self.processor.process_historical_revisions_response(book_data, meta)
                        if snapshots:
                            self.storage.save_snapshots(snapshots)
                            
                # æˆåŠŸæ¨è¿›ä¸€å¤©
                self._update_state(state, last_archived_time=day_end, status="running")
                curr = day_end
                
            except Exception as e:
                logger.error(f"[{area}] å†å²å½’æ¡£å¤±è´¥ ({target_date_str}): {e}")
                self._update_state(state, status="error", last_error=str(e))
                return

    def sync_realtime_stream(self, area: str, return_ticks: bool = False):
        """
        ã€å®æ—¶åŒæ­¥ã€‘
        ç­–ç•¥ï¼šä½¿ç”¨ OrderRevisions/ByUpdatedTime è¿½èµ¶æœ€æ–° Ticks
        """
        state = self._get_or_create_state(area)
        now = datetime.utcnow()
        
        # å¼‚å¸¸çŠ¶æ€é‡ç½®é€»è¾‘
        safe_start_limit = now - timedelta(hours=48)
        start_time = state.last_realtime_time
        
        if start_time < safe_start_limit:
            start_time = safe_start_limit
            logger.warning(f"[{area}] å®æ—¶è¿›åº¦è½åå¤ªä¹…ï¼Œé‡ç½®ä¸º 48 å°æ—¶å‰")
        elif start_time > now:
            start_time = now - timedelta(hours=2)
            logger.warning(f"[{area}] å®æ—¶è¿›åº¦å¼‚å¸¸(è¶…å‰)ï¼Œé‡ç½®ä¸º 2 å°æ—¶å‰")

        if start_time >= now - timedelta(minutes=1):
            return [] if return_ticks else None 

        logger.info(f"[{area}] ğŸš€ å¯åŠ¨å®æ—¶åŒæ­¥ (API B): {start_time} -> {now}")
        
        all_new_ticks = []
        try:
            total_saved = 0
            # ä½¿ç”¨æµå¼ Fetcher æ¶ˆè´¹ API B
            for chunk in self.fetcher.fetch_recent_orders(area, start_time, now):
                ticks = self.processor.process_recent_orders_response(chunk)
                if ticks:
                    self.storage.save_ticks(ticks)
                    total_saved += len(ticks)
                
                if return_ticks:
                    all_new_ticks.extend(ticks)
            
            self._update_state(state, last_realtime_time=now, status="ok")
            if total_saved > 0:
                logger.info(f"[{area}] å®æ—¶åŒæ­¥å®Œæˆï¼Œå…¥åº“ {total_saved} æ¡ Ticks")

            return all_new_ticks if return_ticks else None
            
        except Exception as e:
            logger.error(f"[{area}] å®æ—¶åŒæ­¥å¤±è´¥: {e}")
            self._update_state(state, status="warning", last_error=str(e))
            return [] if return_ticks else None

    def sync_all(self):
        for area in AUTO_AREAS:
            try:
                self.sync_history_backfill(area)
                self.sync_realtime_stream(area)
            except Exception as e:
                logger.error(f"[{area}] Manager Loop Error: {e}")