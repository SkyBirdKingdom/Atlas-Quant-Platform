# backend/services/order_flow/manager.py
import logging
from datetime import datetime, timedelta, timezone
from sqlalchemy.orm import Session
from ...models import OrderFlowSyncState
from .fetcher import OrderFlowFetcher
from .processor import OrderFlowProcessor
from .storage import OrderFlowService

logger = logging.getLogger("OrderFlowManager")

# è®¢å•åˆå§‹å›æº¯æ—¶é—´ (ä»…å½“æ•°æ®åº“æ— è®°å½•æ—¶ä½¿ç”¨ï¼Œé€šå¸¸ç”± Manager å†…éƒ¨é€»è¾‘å¤„ç†ï¼Œè¿™é‡Œä½œä¸ºå¤‡æ³¨æˆ–ä¼ é€’å‚æ•°)
INITIAL_START_DATE = "2025-10-01T00:00:00"
class OrderFlowManager:
    def __init__(self, db: Session):
        self.db = db
        self.fetcher = OrderFlowFetcher()
        self.processor = OrderFlowProcessor()
        self.storage = OrderFlowService(db)
    
    def _get_or_create_state(self, area: str) -> OrderFlowSyncState:
        state = self.db.query(OrderFlowSyncState).filter(OrderFlowSyncState.area == area).first()
        if not state:
            start_time = datetime.fromisoformat(INITIAL_START_DATE)
            # å®æ—¶æŒ‡é’ˆé»˜è®¤å›æº¯1å°æ—¶
            realtime_start = datetime.utcnow() - timedelta(hours=1)
            
            state = OrderFlowSyncState(
                area=area, 
                last_archived_time=start_time,
                last_realtime_time=realtime_start 
            )
            self.db.add(state)
            self.db.commit()
            logger.info(f"[{area}] åˆå§‹åŒ–åŒæ­¥çŠ¶æ€: é»˜è®¤å›æº¯1å°æ—¶")
        return state
    
    def _update_state(self, state, **kwargs):
        for k, v in kwargs.items():
            setattr(state, k, v)
        state.updated_at = datetime.utcnow()
        self.db.commit()

    # --- æ ¸å¿ƒçŠ¶æ€ç®¡ç† ---
    def _update_checkpoint(self, area: str, new_time: datetime):
        """æ›´æ–°æ–­ç‚¹æ—¶é—´"""
        state = self.db.query(OrderFlowSyncState).filter_by(area=area).first()
        if not state:
            state = OrderFlowSyncState(area=area, last_realtime_time=new_time)
            self.db.add(state)
        else:
            # åªæœ‰å½“æ–°æ—¶é—´å¤§äºæ—§æ—¶é—´æ—¶æ‰æ›´æ–°ï¼Œé˜²æ­¢å›æ»š
            if new_time > state.last_realtime_time:
                state.last_realtime_time = new_time
        self.db.commit()
    
    def sync_history_backfill(self, area: str):
        """
        ã€å†å²å½’æ¡£ã€‘
        ç­–ç•¥ï¼šç”±äº OrderBook/ByContractId æœ‰ 14-38 å°æ—¶å»¶è¿Ÿï¼Œ
        æˆ‘ä»¬åªå½’æ¡£ '48å°æ—¶å‰' çš„æ•°æ®ï¼Œç¡®ä¿æ•°æ®å·²å°±ç»ªã€‚
        """
        state = self._get_or_create_state(area)
        
        # å½’æ¡£çº¿ï¼šè®¾ç½®ä¸º 48 å°æ—¶å‰ (é¿å¼€ 38h çš„æœ€å¤§å»¶è¿Ÿ)
        archive_limit = datetime.utcnow() - timedelta(hours=48)
        
        curr = state.last_archived_time
        curr = curr.replace(hour=0, minute=0, second=0, microsecond=0)
        if curr >= archive_limit:
            return

        logger.info(f"[{area}] ğŸ“š å¯åŠ¨å†å²å½’æ¡£ (API A): {curr.date()} -> {archive_limit.date()}")
        
        while curr < archive_limit:
            # æ¯æ¬¡å¤„ç†ä¸€å¤©
            day_start = curr
            day_end = curr + timedelta(days=1)
            
            # ä¸ºäº†é˜²æ­¢æ­»å¾ªç¯ï¼Œè‹¥ day_end è¶…è¿‡ limit åˆ™æˆªæ–­
            if day_end > archive_limit:
                break
            
            target_date_str = day_start.strftime('%Y-%m-%d')
            
            try:
                # 1. è·å–è¯¥æ—¥æœŸçš„åˆçº¦åˆ—è¡¨
                # API: OrderBook/ContractsIds/ByArea
                contract_resp = self.fetcher.fetch_contract_list(area, target_date_str)
                contracts = contract_resp.get('contracts', [])
                
                if contracts:
                    logger.info(f"[{area}] {target_date_str} å…±æœ‰ {len(contracts)} ä¸ªåˆçº¦éœ€å½’æ¡£")
                    
                    for c in contracts:
                        cid = c.get('contractId') or c.get('id')
                        
                        # 2. æŠ“å–è¯¥åˆçº¦çš„å®Œæ•´å†å² (å« Snapshots)
                        # API: OrderBook/ByContractId
                        book_data = self.fetcher.fetch_historical_revisions(area, cid, target_date_str)
                        
                        # 3. æ„é€ å…ƒæ•°æ® (API A å¯èƒ½ä¸è¿”å› delivery areaï¼Œéœ€æ‰‹åŠ¨è¡¥å…¨)
                        meta = {
                            'contract_name': c.get('contractName'),
                            'delivery_start': c.get('deliveryStart'),
                            'delivery_end': c.get('deliveryEnd'),
                            'delivery_area': area
                        }
                        
                        # 4. è§£æå¹¶å…¥åº“ (é‡ç‚¹æ˜¯ Snapshots)
                        result = self.processor.process_historical_revisions_response(book_data, meta)
                        snapshots = result.get('snapshots', [])
                        if snapshots:
                            self.storage.save_snapshots(snapshots)
                        
                        # 2. ä¿å­˜ Ticks (è¿™å°±æ˜¯ä½ ç¼ºå¤±çš„æ•°æ®!)
                        ticks = result.get('ticks', [])
                        if ticks:
                            self.storage.save_ticks(ticks)
                            
                # æˆåŠŸæ¨è¿›ä¸€å¤©
                self._update_state(state, last_archived_time=day_end, status="running")
                curr = day_end
                
            except Exception as e:
                logger.error(f"[{area}] å†å²å½’æ¡£å¤±è´¥ ({target_date_str}): {e}")
                self._update_state(state, status="error", last_error=str(e))
                return

    # --- è‡ªåŠ¨åŒæ­¥é€»è¾‘ ---
    def sync_realtime(self, area: str):
        """
        ã€æ–­ç‚¹ç»­ä¼ ã€‘å®æ—¶åŒæ­¥ä»»åŠ¡
        """
        # 1. è·å–æ–­ç‚¹
        state = self._get_or_create_state(area)
        last_time = state.last_realtime_time
        # ä¸ºäº†é˜²æ­¢æ¯«ç§’çº§çš„æ—¶é—´è¾¹ç•Œä¸¢å¤±ï¼Œå‘å‰å›æº¯ 1 åˆ†é’Ÿï¼ˆOverlapï¼‰
        # ä¾èµ– Storage å±‚çš„å»é‡æœºåˆ¶å¤„ç†é‡å¤æ•°æ®
        fetch_start = last_time - timedelta(minutes=1)
        fetch_end = datetime.utcnow()

        logger.info(f"[{area}] å¯åŠ¨å¢é‡åŒæ­¥: {fetch_start} -> {fetch_end}")

        try:
            count = 0
            # 2. è°ƒç”¨ Fetcher (æµå¼è·å–ï¼Œè‡ªåŠ¨å¤„ç† 4å°æ—¶é™åˆ¶)
            # API: updatedTimeFrom - updatedTimeTo [cite: 35, 60-61]
            for chunk_data in self.fetcher.fetch_recent_orders(area, fetch_start, fetch_end):
                # 3. å¤„ç†æ•°æ®
                ticks = self.processor.process_api_response(chunk_data, source_type="Stream")
                
                # 4. å¹‚ç­‰å…¥åº“ (Storage å±‚å¤„ç†å»é‡)
                if ticks:
                    self.storage.save_ticks(ticks)
                    count += len(ticks)
            
            # 5. æ›´æ–°æ–­ç‚¹ (åªæœ‰æˆåŠŸæ‰æ›´æ–°)
            self._update_checkpoint(area, fetch_end)
            logger.info(f"[{area}] åŒæ­¥å®Œæˆï¼Œå…¥åº“ {count} æ¡ Ticksï¼Œæ–­ç‚¹æ›´æ–°è‡³ {fetch_end}")

        except Exception as e:
            logger.error(f"[{area}] åŒæ­¥ä¸­æ–­: {e}")
            # å‘ç”Ÿå¼‚å¸¸æ—¶ä¸æ›´æ–° checkpointï¼Œä¸‹æ¬¡ä»»åŠ¡ä¼šè‡ªåŠ¨é‡è¯•è¿™æ®µæ—¶é—´

    # --- æ‰‹åŠ¨è¡¥å½•é€»è¾‘ ---
    def manual_backfill_range(self, area: str, start_str: str, end_str: str):
        """
        ã€æ‰‹åŠ¨è¡¥å½•ã€‘å¼ºåˆ¶æŠ“å–æŒ‡å®šæ—¶é—´æ®µ
        åœºæ™¯ï¼šå‘ç°æŸå¤©æ•°æ®ç¼ºå¤±ï¼Œæˆ–éœ€è¦è¡¥å……å†å²æ•°æ®
        """
        start = datetime.fromisoformat(start_str)
        end = datetime.fromisoformat(end_str)
        
        logger.info(f"[{area}] âš ï¸ å¼€å§‹æ‰‹åŠ¨è¡¥å½•: {start} -> {end}")
        
        try:
            total = 0
            # å¤ç”¨ Fetcher çš„é€»è¾‘ï¼Œå®ƒå·²ç»å¤„ç†äº†åˆ†é¡µå’Œåˆ‡ç‰‡
            for chunk_data in self.fetcher.fetch_recent_orders(area, start, end):
                ticks = self.processor.process_api_response(chunk_data, source_type="ManualBackfill")
                self.storage.save_ticks(ticks)
                total += len(ticks)
                
            logger.info(f"[{area}] æ‰‹åŠ¨è¡¥å½•å®Œæˆï¼Œå…±æ¢å¤ {total} æ¡æ•°æ®")
        except Exception as e:
            logger.error(f"[{area}] æ‰‹åŠ¨è¡¥å½•å¤±è´¥: {e}")
            raise