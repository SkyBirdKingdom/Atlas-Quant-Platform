# backend/services/order_flow/manager.py
import logging
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from sqlalchemy.orm import Session
from ...models import OrderFlowSyncState, OrderContract
from .fetcher import OrderFlowFetcher
from .processor import OrderFlowProcessor
from .storage import OrderFlowService
from ...database import SessionLocal
import gc

logger = logging.getLogger("OrderFlowManager")

# è®¢å•åˆå§‹å›æº¯æ—¶é—´ (ä»…å½“æ•°æ®åº“æ— è®°å½•æ—¶ä½¿ç”¨ï¼Œé€šå¸¸ç”± Manager å†…éƒ¨é€»è¾‘å¤„ç†ï¼Œè¿™é‡Œä½œä¸ºå¤‡æ³¨æˆ–ä¼ é€’å‚æ•°)
INITIAL_START_DATE = "2025-10-01T00:00:00"
class OrderFlowManager:
    def __init__(self, db: Session):
        self.db = db
        self.fetcher = OrderFlowFetcher()
        self.processor = OrderFlowProcessor()
        self.storage = OrderFlowService(db)
    
    def _get_or_create_state(self, area: str) -> OrderFlowSyncState:
        state = self.db.query(OrderFlowSyncState).filter(OrderFlowSyncState.area == area).first()
        if not state:
            start_time = datetime.fromisoformat(INITIAL_START_DATE)
            # å®æ—¶æŒ‡é’ˆé»˜è®¤å›æº¯1å°æ—¶
            realtime_start = datetime.now(timezone.utc) - timedelta(hours=1)
            
            state = OrderFlowSyncState(
                area=area, 
                last_archived_time=start_time,
                last_realtime_time=realtime_start 
            )
            self.db.add(state)
            self.db.commit()
            logger.info(f"[{area}] åˆå§‹åŒ–åŒæ­¥çŠ¶æ€: é»˜è®¤å›æº¯1å°æ—¶")
        return state
    
    def _update_state(self, state, **kwargs):
        for k, v in kwargs.items():
            if isinstance(v, datetime) and v.tzinfo is None:
                v = v.replace(tzinfo=timezone.utc)
            setattr(state, k, v)
        state.updated_at = datetime.now(timezone.utc)
        self.db.commit()

    # --- æ ¸å¿ƒçŠ¶æ€ç®¡ç† ---
    def _update_checkpoint(self, area: str, new_time: datetime):
        """æ›´æ–°æ–­ç‚¹æ—¶é—´"""
        state = self.db.query(OrderFlowSyncState).filter_by(area=area).first()
        if not state:
            state = OrderFlowSyncState(area=area, last_realtime_time=new_time)
            self.db.add(state)
        else:
            # åªæœ‰å½“æ–°æ—¶é—´å¤§äºæ—§æ—¶é—´æ—¶æ‰æ›´æ–°ï¼Œé˜²æ­¢å›æ»š
            if new_time > state.last_realtime_time:
                state.last_realtime_time = new_time
        self.db.commit()
    
    def _process_single_contract(self, area: str, date_str: str, contract_info: dict, is_cold: bool):
        """
        [çº¿ç¨‹ä»»åŠ¡] å¤„ç†å•ä¸ªåˆçº¦ï¼šä¸‹è½½ -> è§£æ -> å­˜å‚¨ (DB æˆ– Parquet)
        """
        cid = contract_info.get('contract_id')

        # æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„ DB Session (ä»…å½“éœ€è¦å†™å…¥ DB æ—¶)
        thread_db = SessionLocal()
        thread_storage = OrderFlowService(thread_db)
        
        try:
            # 1. ä¸‹è½½å…¨é‡å†å²
            book_data = self.fetcher.fetch_historical_revisions(area, cid, date_str)
            
            ticks = self.processor.process_historical_revisions_response(book_data)

            del book_data
            book_data = None
            
            if ticks:
                # 3. å­˜å‚¨ (å†·çƒ­åˆ†ç¦»)
                if is_cold:
                    # [Cold] å­˜ä¸º Parquet æ–‡ä»¶ (ä¸å ç”¨ DB è¿æ¥)
                    thread_storage.save_ticks_to_parquet(ticks, area, date_str, cid)
                else:
                    # [Hot] å­˜å…¥ PostgreSQL
                    thread_storage.save_ticks(ticks)
            
            # 4. ã€å…³é”®ã€‘æ ‡è®°è¯¥åˆçº¦å·²å®Œæˆ
            thread_storage.mark_contract_archived(cid)
            count = len(ticks) if ticks else 0
            return True, count

        except Exception as e:
            logger.error(f"åˆçº¦ {cid} å¤„ç†å¤±è´¥: {e}")
            return False, 0
        finally:
            thread_db.close()
            del ticks
            del thread_storage
            del thread_db
            gc.collect()
    
    def sync_history_backfill(self, area: str):
        """
        ã€å¹¶å‘ç‰ˆ T+1 å†å²å½’æ¡£ã€‘
        1. å¹¶å‘å¤„ç†ï¼šä½¿ç”¨ ThreadPoolExecutor
        2. å†·çƒ­åˆ†ç¦»ï¼š>7å¤©å­˜æ–‡ä»¶ï¼Œ<=7å¤©å­˜æ•°æ®åº“
        """
        state = self._get_or_create_state(area)
        
        # å½’æ¡£ç»“æŸç‚¹ï¼š48å°æ—¶å‰ (ç¡®ä¿æ•°æ®å°±ç»ª)
        archive_limit = datetime.now(timezone.utc) - timedelta(hours=48)
        
        # å†·çƒ­åˆ†ç•Œçº¿ï¼š7å¤©å‰
        hot_cold_threshold = datetime.now(timezone.utc) - timedelta(days=7)

        last_archived = state.last_archived_time
        if last_archived and last_archived.tzinfo is None:
            last_archived = last_archived.replace(tzinfo=timezone.utc)

        # å¯¹é½æ—¶é—´åˆ° 00:00:00
        curr = last_archived or (datetime.now(timezone.utc) - timedelta(days=8))
        curr = curr.replace(hour=0, minute=0, second=0, microsecond=0)

        if curr >= archive_limit:
            return

        logger.info(f"[{area}] ğŸ“š å¯åŠ¨é«˜å¹¶å‘å½’æ¡£: {curr.date()} -> {archive_limit.date()}")
        
        # å»ºè®®çº¿ç¨‹æ•°ï¼šCPUæ ¸å¿ƒæ•° * 2 æˆ– 4ï¼Œæˆ–è€…å›ºå®š 10 (ç½‘ç»œIOå¯†é›†å‹)
        MAX_WORKERS = 10 

        try:
            while curr < archive_limit:
                target_date_str = curr.strftime('%Y-%m-%d')
                
                # åˆ¤æ–­å†·çƒ­ (æ¯”è¾ƒ delivery start date ä¸ å½“å‰æ—¶é—´)
                # curr ä»£è¡¨ delivery date
                is_cold = curr < hot_cold_threshold
                mode_str = "â„ï¸ COLD (Parquet)" if is_cold else "ğŸ”¥ HOT (DB)"
                logger.info(f"[{area}] å¤„ç†æ—¥æœŸ {target_date_str} æ¨¡å¼: {mode_str}")

                # 1. è·å–åˆçº¦åˆ—è¡¨ (ä¸»çº¿ç¨‹æ‰§è¡Œ)
                try:
                    contract_resp = self.fetcher.fetch_contract_list(area, target_date_str)
                    
                    # é¡ºä¾¿ä¿å­˜åˆçº¦å…ƒæ•°æ® (å­˜ DB)
                    contracts_meta = self.processor.process_contracts_response(contract_resp)
                    self.storage.save_contracts(contracts_meta)
                    logger.info(f"[{area}] {target_date_str} è·å–åˆçº¦åˆ—è¡¨ï¼Œå…± {len(contracts_meta)} ä¸ªåˆçº¦")
                    
                except Exception as e:
                    logger.error(f"[{area}] è·å–åˆçº¦åˆ—è¡¨å¤±è´¥ ({target_date_str}): {e}")
                    break
                
                # --- Step 2: æŸ¥è¯¢å‰©ä½™ä»»åŠ¡ (Filter) ---
                # ä» DB æŸ¥å‡ºã€å½“å¤©ã€‘ä¸”ã€æœªå½’æ¡£ã€‘çš„åˆçº¦
                pending_contracts = self.db.query(OrderContract).filter(
                    OrderContract.delivery_area == area,
                    OrderContract.delivery_date_utc == curr.date(),
                    OrderContract.is_archived == False
                ).all()

                # å°† SQLAlchemy å¯¹è±¡è½¬ä¸º Dictï¼Œæ–¹ä¾¿ä¼ å…¥çº¿ç¨‹
                pending_list = [
                    {"contract_id": c.contract_id, "contract_name": c.contract_name} 
                    for c in pending_contracts
                ]
                
                total_pending = len(pending_list)

                if total_pending == 0:
                    logger.info(f"[{area}] {target_date_str} æ‰€æœ‰åˆçº¦å·²å½’æ¡£ï¼Œæ¨è¿›æ—¥æœŸã€‚")
                    curr += timedelta(days=1)
                    self._update_state(state, last_archived_time=curr)
                    continue
                
                logger.info(f"[{area}] {target_date_str} å‰©ä½™ {total_pending} ä¸ªåˆçº¦å¾…å¤„ç† ({'Cold' if is_cold else 'Hot'})")

                # --- Step 3: å¹¶å‘æ‰§è¡Œ (Execute) ---
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    future_to_cid = {
                        executor.submit(
                            self._process_single_contract, 
                            area, target_date_str, c_info, is_cold
                        ): c_info['contract_id'] for c_info in pending_list
                    }
                    
                    completed_in_batch = 0
                    
                    for future in as_completed(future_to_cid):
                        cid = future_to_cid[future]
                        try:
                            success, count = future.result()
                            if success:
                                completed_in_batch += 1
                        except Exception as exc:
                            logger.error(f"ä»»åŠ¡å¼‚å¸¸ {cid}: {exc}")
                
                # --- Step 4: æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆ (Check) ---
                # å†æ¬¡æŸ¥è¯¢ DBï¼Œçœ‹æ˜¯å¦è¿˜æœ‰å‰©ä½™
                remaining = self.db.query(OrderContract).filter(
                    OrderContract.delivery_area == area,
                    OrderContract.delivery_date_utc == curr.date(),
                    OrderContract.is_archived == False
                ).count()

                if remaining == 0:
                    logger.info(f"[{area}] âœ… {target_date_str} å®Œæˆ (æœ¬æ‰¹ {completed_in_batch})")
                    # åªæœ‰å½“å¤©å…¨éƒ¨æå®šï¼Œæ‰æ¨è¿›å…¨å±€æŒ‡é’ˆ
                    curr += timedelta(days=1)
                    self._update_state(state, last_archived_time=curr)
                else:
                    logger.warning(f"[{area}] âš ï¸ {target_date_str} ä»æœ‰ {remaining} ä¸ªå¤±è´¥/æœªå®Œæˆï¼Œæš‚ä¸æ¨è¿›æ—¥æœŸï¼Œä¸‹æ¬¡é‡è¯•ã€‚")
                    # å¦‚æœæœ‰å¤±è´¥ï¼Œè·³å‡ºæœ¬æ¬¡å¤§å¾ªç¯ï¼Œæˆ–è€… sleep ä¸€ä¸‹å†è¯•
                    # è¿™é‡Œé€‰æ‹© breakï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡ Scheduler è°ƒåº¦å†é‡è¯•ï¼Œé¿å…æ­»å¾ªç¯è½°ç‚¸ API
                    break

                
            logger.info(f"[{area}] âœ… å†å²å½’æ¡£å…¨éƒ¨å®Œæˆ")

        except Exception as e:
            logger.error(f"[{area}] å½’æ¡£ä¸»æµç¨‹å¼‚å¸¸: {e}")

    # --- è‡ªåŠ¨åŒæ­¥é€»è¾‘ ---
    def sync_realtime(self, area: str):
        """
        ã€æ–­ç‚¹ç»­ä¼ ã€‘å®æ—¶åŒæ­¥ä»»åŠ¡
        """
        # 1. è·å–æ–­ç‚¹
        state = self._get_or_create_state(area)
        last_time = state.last_realtime_time
        # ä¸ºäº†é˜²æ­¢æ¯«ç§’çº§çš„æ—¶é—´è¾¹ç•Œä¸¢å¤±ï¼Œå‘å‰å›æº¯ 1 åˆ†é’Ÿï¼ˆOverlapï¼‰
        # ä¾èµ– Storage å±‚çš„å»é‡æœºåˆ¶å¤„ç†é‡å¤æ•°æ®
        fetch_start = last_time - timedelta(minutes=1)
        fetch_end = datetime.utcnow()

        logger.info(f"[{area}] å¯åŠ¨å¢é‡åŒæ­¥: {fetch_start} -> {fetch_end}")

        try:
            count = 0
            # 2. è°ƒç”¨ Fetcher (æµå¼è·å–ï¼Œè‡ªåŠ¨å¤„ç† 4å°æ—¶é™åˆ¶)
            # API: updatedTimeFrom - updatedTimeTo [cite: 35, 60-61]
            for chunk_data in self.fetcher.fetch_recent_orders(area, fetch_start, fetch_end):
                # 3. å¤„ç†æ•°æ®
                ticks = self.processor.process_api_response(chunk_data, source_type="Stream")
                
                # 4. å¹‚ç­‰å…¥åº“ (Storage å±‚å¤„ç†å»é‡)
                if ticks:
                    self.storage.save_ticks(ticks)
                    count += len(ticks)
                del ticks
                gc.collect()
            
            # 5. æ›´æ–°æ–­ç‚¹ (åªæœ‰æˆåŠŸæ‰æ›´æ–°)
            self._update_checkpoint(area, fetch_end)
            logger.info(f"[{area}] åŒæ­¥å®Œæˆï¼Œå…¥åº“ {count} æ¡ Ticksï¼Œæ–­ç‚¹æ›´æ–°è‡³ {fetch_end}")

        except Exception as e:
            logger.error(f"[{area}] åŒæ­¥ä¸­æ–­: {e}")
            # å‘ç”Ÿå¼‚å¸¸æ—¶ä¸æ›´æ–° checkpointï¼Œä¸‹æ¬¡ä»»åŠ¡ä¼šè‡ªåŠ¨é‡è¯•è¿™æ®µæ—¶é—´

    # --- æ‰‹åŠ¨è¡¥å½•é€»è¾‘ ---
    def manual_backfill_range(self, area: str, start_str: str, end_str: str):
        """
        ã€æ‰‹åŠ¨è¡¥å½•ã€‘å¼ºåˆ¶æŠ“å–æŒ‡å®šæ—¶é—´æ®µ
        åœºæ™¯ï¼šå‘ç°æŸå¤©æ•°æ®ç¼ºå¤±ï¼Œæˆ–éœ€è¦è¡¥å……å†å²æ•°æ®
        """
        start = datetime.fromisoformat(start_str)
        end = datetime.fromisoformat(end_str)
        
        logger.info(f"[{area}] âš ï¸ å¼€å§‹æ‰‹åŠ¨è¡¥å½•: {start} -> {end}")
        
        try:
            total = 0
            # å¤ç”¨ Fetcher çš„é€»è¾‘ï¼Œå®ƒå·²ç»å¤„ç†äº†åˆ†é¡µå’Œåˆ‡ç‰‡
            for chunk_data in self.fetcher.fetch_recent_orders(area, start, end):
                ticks = self.processor.process_api_response(chunk_data, source_type="ManualBackfill")
                self.storage.save_ticks(ticks)
                total += len(ticks)
                
            logger.info(f"[{area}] æ‰‹åŠ¨è¡¥å½•å®Œæˆï¼Œå…±æ¢å¤ {total} æ¡æ•°æ®")
        except Exception as e:
            logger.error(f"[{area}] æ‰‹åŠ¨è¡¥å½•å¤±è´¥: {e}")
            raise