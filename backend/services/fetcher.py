import logging
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.orm import Session
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from ..models import Trade, FetchState
from ..core.config import settings
import gc
from dateutil import parser as date_parser


logger = logging.getLogger("NordPoolFetcher")

# é…ç½®
AUTO_AREAS = ["SE1", "SE2", "SE3", "SE4"]  # è‡ªåŠ¨åŒæ­¥çš„åŒºåŸŸåˆ—è¡¨
API_URL = "https://data-api.nordpoolgroup.com/api/v2/Intraday/Trades/ByDeliveryStart"

# --- 1. ç½‘ç»œå±‚ï¼šå¸¦è‡ªåŠ¨é‡è¯•çš„ API è¯·æ±‚ ---

# === é…ç½®åŒº ===
# é¦–æ¬¡è¿è¡Œæ—¶ï¼Œå¦‚æœæ²¡æœ‰å†å²è®°å½•ï¼Œä»è¿™ä¸ªæ—¶é—´ç‚¹å¼€å§‹æŠ“å–
# æ³¨æ„ï¼šæ ¼å¼å¿…é¡»æ˜¯ ISO 8601
INITIAL_START_DATE = "2025-01-01T00:00:00Z"
user = settings.NORDPOOL_USER
pwd = settings.NORDPOOL_PASSWORD

def get_token():
    token_url = "https://sts.nordpoolgroup.com/connect/token"
    headers = {"Content-Type": "application/x-www-form-urlencoded", "Authorization": "Basic Y2xpZW50X21hcmtldGRhdGFfYXBpOmNsaWVudF9tYXJrZXRkYXRhX2FwaQ=="}
    params = {"grant_type": "password", "scope": "marketdata_api", "username": user, "password": pwd}
    resp = requests.post(token_url, headers=headers, data=params, timeout=10)
    resp.raise_for_status()
    return resp.json().get("access_token")

# ä½¿ç”¨è£…é¥°å™¨å¤„ç†é‡è¯•ï¼š
# - æœ€å¤šé‡è¯• 5 æ¬¡
# - ç­‰å¾…æ—¶é—´æŒ‡æ•°å¢é•¿ (2s, 4s, 8s...)ï¼Œé˜²æ­¢æŠŠ NordPool å†²å®
# - åªåœ¨é‡åˆ° RequestException (ç½‘ç»œé”™, 500, 502) æ—¶é‡è¯•
@retry(
    stop=stop_after_attempt(5), 
    wait=wait_exponential(multiplier=1, min=2, max=60),
    retry=retry_if_exception_type(requests.RequestException)
)
def fetch_api_chunk(token, area, start_str, end_str):
    params = {"deliveryStartFrom": start_str, "deliveryStartTo": end_str, "areas": area}
    headers = {"accept": "application/json", "authorization": f"Bearer {token}"}
    
    resp = requests.get(API_URL, params=params, headers=headers, timeout=30)
    
    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœ Token è¿‡æœŸ (401)ï¼ŒæŠ›å‡ºé”™è¯¯è®©å¤–å±‚åˆ·æ–° Token
    if resp.status_code == 401:
        raise PermissionError("Token expired")
    
    resp.raise_for_status() # é 200 æŠ›å‡ºå¼‚å¸¸ï¼Œè§¦å‘é‡è¯•
    return resp.json()

# --- 2. æ•°æ®å¤„ç†ä¸å­˜å‚¨ ---

def flatten_and_parse(raw_data, area):
    """
    è§£æ API æ•°æ®ï¼Œå°†å…¶æ‰å¹³åŒ–ã€‚
    å…³é”®é€»è¾‘ï¼šéå† legsï¼Œåªæå– belongs to area çš„ leg ç”Ÿæˆè®°å½•ã€‚
    """
    rows = []
    
    # API è¿”å›ç»“æ„: contracts -> trades -> legs
    contracts = raw_data.get('contracts', []) or []
    
    for contract in contracts:
        # åˆçº¦å±‚çº§ä¿¡æ¯
        contract_base = {
            'contractId': contract.get('contractId'),
            'contractName': contract.get('contractName'),
            'deliveryStart': contract.get('deliveryStart'),
            'deliveryEnd': contract.get('deliveryEnd'),
        }
        
        trades = contract.get('trades', []) or []
        for trade in trades:
            # äº¤æ˜“å±‚çº§ä¿¡æ¯
            trade_base = {
                'tradeId': trade.get('tradeId'),
                'tradeTime': trade.get('tradeTime'),
                'tradeUpdatedAt': trade.get('tradeUpdatedAt'),
                'tradeState': trade.get('tradeState'),
                'revisionNumber': trade.get('revisionNumber'),
                'price': trade.get('price'),
                'volume': trade.get('volume'),
                'tradePhase': trade.get('tradePhase'),
                'crossPx': trade.get('crossPx'),
            }
            
            legs = trade.get('legs') or []
            
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šLeg æ‹†è§£ ---
            # åªæœ‰å½“ leg çš„ deliveryArea ç­‰äºå½“å‰æŠ“å–çš„ area æ—¶ï¼Œæ‰ç”Ÿæˆè®°å½•
            # è¿™æ ·å½“æŠ“å– SE2 æ—¶å­˜ SE2 çš„è…¿ï¼ŒæŠ“å– SE3 æ—¶å­˜ SE3 çš„è…¿ï¼Œäº’ä¸å†²çª
            for leg in legs:
                leg_area = leg.get('deliveryArea')
                
                rows.append({
                    **contract_base,
                    **trade_base,
                    'deliveryArea': leg_area,
                    'referenceOrderId': leg.get('referenceOrderId'),
                    'tradeSide': leg.get('tradeSide') # Buy or Sell
                })
            
            # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœ API è¿”å›è€æ—§æ•°æ®æ²¡æœ‰ legs å­—æ®µï¼Œä½†å±äºè¯¥åŒºåŸŸ
            # (è¿™ç§æƒ…å†µè¾ƒå°‘è§ï¼Œä½†ä¸ºäº†å¥å£®æ€§ä¿ç•™)
            if not legs:
                # è¿™é‡Œå‡è®¾å¦‚æœæ²¡æœ‰ legs ç»†åˆ†ï¼Œå°±å½“ä½œä¸€æ¡é€šç”¨è®°å½•ï¼Œ
                # ä½†ä¸»é”®éœ€è¦ tradeSideï¼Œæˆ‘ä»¬ç»™ä¸ªé»˜è®¤å€¼ 'Unknown' é˜²æ­¢æŠ¥é”™
                 rows.append({
                    **contract_base,
                    **trade_base,
                    'deliveryArea': area,
                    'referenceOrderId': None,
                    'tradeSide': 'Unknown' 
                })

    return rows

def save_chunk_to_db(db: Session, data_list: list):
    if not data_list: return
    
    # df = pd.DataFrame(data_list)
    db_records = []

    for r in data_list:
        # 1. çº¯ Python è§£ææ—¶é—´ (æ¯” Pandas å¿«ä¸”ä¸å å†…å­˜)
        # å‡è®¾ API è¿”å›çš„æ˜¯ ISO æ ¼å¼å­—ç¬¦ä¸²
        d_start = r.get('deliveryStart')
        d_end = r.get('deliveryEnd')
        
        # ç®€å•çš„ ISO è§£æè¾…åŠ©å‡½æ•° (å…¼å®¹æ€§å¤„ç†)
        def parse_ts(ts_str):
            if not ts_str: return None
            try:
                # å°è¯•ç”¨æœ€é«˜æ•ˆçš„æ–¹å¼è§£æï¼Œå¦‚æœæ˜¯ Python 3.11+ å¯ä»¥ç›´æ¥ fromisoformat
                return datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
            except:
                # å…œåº•æ–¹æ¡ˆ
                try:
                    return date_parser.parse(ts_str)
                except:
                    return None

        dt_start = parse_ts(d_start)
        dt_end = parse_ts(d_end)
        
        # 2. è®¡ç®— duration å’Œ contract_type
        duration = 0.0
        c_type = 'Unknown'
        
        if dt_start and dt_end:
            # total_seconds() / 60
            duration = (dt_end - dt_start).total_seconds() / 60.0
            
            # é€»è¾‘åŒåŸ Pandas np.select
            if abs(duration - 60) < 1:
                c_type = 'PH'
            elif abs(duration - 15) < 1:
                c_type = 'QH'
            else:
                c_type = 'Other'

        # 3. æ„å»º DB è®°å½•
        db_record = {
            "trade_id": r.get('tradeId'),
            "delivery_area": r.get('deliveryArea'),
            "trade_side": r.get('tradeSide'),
            
            "contract_id": r.get('contractId'),
            "contract_name": r.get('contractName'),
            "delivery_start": dt_start, # ç›´æ¥ä½¿ç”¨ datetime å¯¹è±¡
            "delivery_end": dt_end,
            "duration_minutes": duration,
            "contract_type": c_type,
            
            "price": r.get('price'),
            "volume": r.get('volume'),
            "trade_time": parse_ts(r.get('tradeTime')),
            "trade_updated_at": parse_ts(r.get('tradeUpdatedAt')),
            
            "trade_state": r.get('tradeState'),
            "revision_number": r.get('revisionNumber'),
            "trade_phase": r.get('tradePhase'),
            "cross_px": r.get('crossPx'),
            "reference_order_id": r.get('referenceOrderId'),
            "created_at": datetime.utcnow()
        }
        db_records.append(db_record)

    # 3. æ‰§è¡Œ Upsert
    if not db_records: return

    try:
        stmt = insert(Trade).values(db_records)
        stmt = stmt.on_conflict_do_update(
            index_elements=['trade_id', 'delivery_area', 'trade_side'],
            set_={
                "trade_updated_at": stmt.excluded.trade_updated_at,
                "trade_state": stmt.excluded.trade_state,
                "revision_number": stmt.excluded.revision_number,
                "price": stmt.excluded.price,
                "volume": stmt.excluded.volume
            }
        )
        db.execute(stmt)
        db.commit()
    except Exception as e:
        logger.error(f"Save DB Error: {e}")
        db.rollback()
    finally:
        del db_records
        del data_list
    
# --- 3. çŠ¶æ€ç®¡ç† ---

def update_fetch_state(db: Session, area: str, last_time=None, status="running", error=None):
    state = db.query(FetchState).filter(FetchState.area == area).first()
    if not state:
        state = FetchState(area=area)
        db.add(state)
    
    if last_time:
        state.last_fetched_time = last_time
    
    state.status = status
    state.updated_at = datetime.utcnow()
    if error:
        state.last_error = str(error)[:500] # æˆªæ–­é”™è¯¯ä¿¡æ¯é˜²æ­¢å¤ªé•¿
    else:
        state.last_error = None # æ¸…é™¤é”™è¯¯
        
    db.commit()

# --- 4. ä¸»åŒæ­¥é€»è¾‘ (å¢å¼ºç‰ˆ) ---

def sync_area_logic(db: Session, area: str):
    """
    ä¿®æ­£åçš„åŒæ­¥é€»è¾‘ï¼š
    1. å†å²æ•°æ®ï¼šæ¨è¿› Checkpointã€‚
    2. æ´»è·ƒæ•°æ®ï¼šæ¯æ¬¡å¼ºåˆ¶é‡åˆ·ï¼Œä¸æ¨è¿› Checkpointã€‚
    """
    # è·å–æ•°æ®åº“é‡Œçš„è¿›åº¦ï¼ˆè¿™æ˜¯â€œå·²å½’æ¡£â€çš„æ—¶é—´çº¿ï¼‰
    state = db.query(FetchState).filter(FetchState.area == area).first()
    
    if state and state.last_fetched_time:
        # åœºæ™¯ A: ä¸æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œæ¥ç€ä¸Šæ¬¡çš„è¿›åº¦è·‘
        archived_time = state.last_fetched_time
    else:
        # åœºæ™¯ B: ç¬¬ä¸€æ¬¡è¿è¡Œ (å†·å¯åŠ¨)ï¼Œä½¿ç”¨é…ç½®çš„åˆå§‹æ—¶é—´
        # å»æ‰ 'Z' å› ä¸º datetime.fromisoformat åœ¨æŸäº› Python ç‰ˆæœ¬å¯¹ Z æ”¯æŒä¸å¥½ï¼Œæˆ–è€…ç»Ÿä¸€ç”¨ replaceå¤„ç†
        archived_time = datetime.fromisoformat(INITIAL_START_DATE.replace('Z', ''))
        logger.info(f"[{area}] é¦–æ¬¡åˆå§‹åŒ–ï¼Œä»é…ç½®æ—¶é—´å¼€å§‹: {archived_time}")
    
    # å®šä¹‰â€œç°åœ¨â€å’Œâ€œæœªæ¥è¾¹ç•Œâ€
    now = datetime.utcnow()
    # æ´»è·ƒçª—å£ï¼šNord Pool æ—¥å†…é€šå¸¸æœ€å¤šå¼€æ”¾åˆ°æ˜å¤© (çº¦ +36~48 å°æ—¶)
    future_limit = now + timedelta(hours=48)
    
    # === ç¬¬ä¸€é˜¶æ®µï¼šè¿½èµ¶å†å² (Backfill) ===
    # è¿™é‡Œçš„ç›®æ ‡æ˜¯æŠŠ Checkpoint æ¨è¿›åˆ° "Now"
    # æˆ‘ä»¬è®¤ä¸ºï¼šDeliveryTime < Now çš„æ•°æ®ï¼ŒåŸºæœ¬å·²ç»ç¨³å®šï¼ˆè™½ç„¶ç†è®ºä¸Šäº¤å‰²å‰éƒ½èƒ½äº¤æ˜“ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾"è¿‡å»"çš„æ—¶é—´æ®µåªéœ€æ¨è¿›ï¼‰
    # æ›´ä¸¥è°¨çš„åšæ³•æ˜¯ï¼šCheckpoint åªèƒ½æ¨è¿›åˆ° "Now - äº¤ä»˜å‘¨æœŸæ—¶é•¿"ï¼Œæ¯”å¦‚ Now - 2å°æ—¶
    
    # è®¾å®šä¸€ä¸ªâ€œå®‰å…¨å½’æ¡£çº¿â€ï¼Œæ¯”å¦‚ 2 å°æ—¶å‰ã€‚åœ¨æ­¤ä¹‹å‰çš„äº¤ä»˜æ—¶æ®µï¼Œæˆ‘ä»¬è®¤ä¸ºæ•°æ®ä¸ä¼šå†å˜äº†ï¼Œå¯ä»¥æ›´æ–° Checkpointã€‚
    safe_archive_line = now - timedelta(hours=2)
    
    curr = archived_time
    token = get_token()
    
    # 1. å¾ªç¯æ¨è¿›å†å²è¿›åº¦
    while curr < safe_archive_line:
        # æ¯æ¬¡æ­¥è¿› 12 å°æ—¶
        chunk_end = min(curr + timedelta(hours=12), safe_archive_line)
        
        t_start = curr.strftime('%Y-%m-%dT%H:%M:%SZ')
        t_end = chunk_end.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        try:
            logger.info(f"[{area}] ğŸ“¥ è¡¥å½•å†å²: {t_start} -> {t_end}")
            
            # API è¯·æ±‚ä¸å…¥åº“
            try:
                raw = fetch_api_chunk(token, area, t_start, t_end)
            except PermissionError:
                token = get_token()
                raw = fetch_api_chunk(token, area, t_start, t_end)
            
            data = flatten_and_parse(raw, area)
            if data:
                save_chunk_to_db(db, data)
                del data # é‡Šæ”¾å†…å­˜
            
            # å…³é”®ï¼šå†å²æ•°æ®æŠ“å®Œä¸€æ®µï¼Œæ›´æ–°ä¸€æ¬¡æ•°æ®åº“ Checkpoint
            update_fetch_state(db, area, last_time=chunk_end, status="running")
            curr = chunk_end
            gc.collect()
            
        except Exception as e:
            logger.error(f"[{area}] å†å²è¡¥å½•å¤±è´¥: {e}")
            update_fetch_state(db, area, status="error", error=str(e))
            return # å†å²éƒ½æŒ‚äº†ï¼Œåé¢å°±åˆ«è·‘äº†
            
    # === ç¬¬äºŒé˜¶æ®µï¼šåˆ·æ–°æ´»è·ƒçª—å£ (Active Window) ===
    # ä» "å®‰å…¨å½’æ¡£çº¿" ä¸€ç›´æŠ“åˆ° "æœªæ¥è¾¹ç•Œ"
    # è¿™éƒ¨åˆ†æ•°æ®ç»å¯¹ã€ä¸èƒ½ã€‘æ›´æ–° Checkpointï¼Œå› ä¸ºä¸‹ä¸€å°æ—¶è¿˜è¦å†æ¥æŠ“ä¸€éæ–°æˆäº¤çš„
    
    active_start = curr # æ¥ç€ä¸Šé¢çš„è¿›åº¦
    logger.info(f"[{area}] ğŸ”„ åˆ·æ–°æ´»è·ƒçª—å£: {active_start.strftime('%Y-%m-%dT%H:%M')} -> {future_limit.strftime('%Y-%m-%dT%H:%M')}")
    
    while active_start < future_limit:
        chunk_end = min(active_start + timedelta(hours=12), future_limit)
        
        t_start = active_start.strftime('%Y-%m-%dT%H:%M:%SZ')
        t_end = chunk_end.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        try:
            # API è¯·æ±‚
            try:
                raw = fetch_api_chunk(token, area, t_start, t_end)
            except PermissionError:
                token = get_token()
                raw = fetch_api_chunk(token, area, t_start, t_end)
            
            data = flatten_and_parse(raw, area)
            
            # å…¥åº“ (åˆ©ç”¨æ•°æ®åº“çš„ ON CONFLICT DO NOTHING å»é‡)
            # è™½ç„¶æˆ‘ä»¬é‡å¤æŠ“å–äº†ï¼Œä½†æ•°æ®åº“é‡Œå·²æœ‰çš„ TradeID ä¼šè¢«å¿½ç•¥ï¼Œåªæœ‰æ–°äº§ç”Ÿçš„ TradeID ä¼šè¢«æ’å…¥
            if data:
                save_chunk_to_db(db, data)
                logger.info(f"[{area}] æ´»è·ƒçª—å£æ›´æ–°: æŠ“å– {len(data)} æ¡")
                del data
            
            # æ³¨æ„ï¼šè¿™é‡Œã€ä¸è°ƒç”¨ã€‘update_fetch_state æ¥æ›´æ–° last_time
            active_start = chunk_end
            gc.collect()
            
        except Exception as e:
            logger.error(f"[{area}] æ´»è·ƒçª—å£åˆ·æ–°å¤±è´¥: {e}")
            # æ´»è·ƒçª—å£å¶å°”å¤±è´¥ä¸å½±å“å¤§å±€ï¼Œè®°å½•é”™è¯¯å³å¯
            update_fetch_state(db, area, status="warning", error=f"Active window error: {e}")
            break

    # å…¨éƒ¨è·‘å®Œï¼ŒçŠ¶æ€æ ‡ä¸º OK
    update_fetch_state(db, area, status="ok")

def sync_all_areas(db: Session):
    """
    å…¥å£å‡½æ•°ï¼šéå†æ‰€æœ‰åŒºåŸŸ
    """
    logger.info("â° å¯åŠ¨å®šæ—¶åŒæ­¥...")
    for area in AUTO_AREAS:
        try:
            sync_area_logic(db, area)
        except Exception as e:
            logger.error(f"âŒ [{area}] ä»»åŠ¡ä¸­æ–­: {e}")
            # è¿™é‡Œ catch ä½ï¼Œä¿è¯ SE3 æŒ‚äº†ä¸å½±å“ SE4 ç»§ç»­è·‘
            continue
    logger.info("âœ… å®šæ—¶åŒæ­¥ç»“æŸ")

def fetch_data_range(db: Session, areas: list, start_date: str, end_date: str):
    """
    æ‰‹åŠ¨è¡¥å½•æŒ‡å®šæ—¶é—´æ®µçš„æ•°æ®ï¼Œä¸å½±å“å…¨å±€åŒæ­¥çŠ¶æ€
    :param db: æ•°æ®åº“ä¼šè¯
    :param areas: åŒºåŸŸä»£ç åˆ—è¡¨ (SE3, SE4)
    :param start_date: å¼€å§‹æ—¶é—´ ISO (2025-01-01)
    :param end_date: ç»“æŸæ—¶é—´ ISO
    """
    try:
        current = datetime.fromisoformat(start_date.replace('Z', ''))
        end = datetime.fromisoformat(end_date.replace('Z', ''))
    except ValueError as e:
        logger.error(f"æ—¶é—´æ ¼å¼é”™è¯¯: {e}")
        raise ValueError("Invalid date format")

    token = get_token()
    total_chunks = 0
    for area in areas:
        logger.info(f"[{area}] ğŸš€ æ‰‹åŠ¨ä»»åŠ¡å¯åŠ¨: {current} -> {end}")

        while current < end:
            # æ‰‹åŠ¨ä»»åŠ¡å…è®¸è·¨åº¦ç¨å¤§ï¼Œæ¯”å¦‚ 24 å°æ—¶
            chunk_end = min(current + timedelta(hours=12), end)
            
            t_start = current.strftime('%Y-%m-%dT%H:%M:%SZ')
            t_end = chunk_end.strftime('%Y-%m-%dT%H:%M:%SZ')
            
            try:
                try:
                    raw = fetch_api_chunk(token, area, t_start, t_end)
                except PermissionError:
                    token = get_token()
                    raw = fetch_api_chunk(token, area, t_start, t_end)
                
                data = flatten_and_parse(raw, area)
                if data:
                    save_chunk_to_db(db, data)
                    logger.info(f"[{area}] æ‰‹åŠ¨å…¥åº“ {len(data)} æ¡ ({t_start})")
                
                current = chunk_end
                total_chunks += 1
                
            except Exception as e:
                logger.error(f"[{area}] æ‰‹åŠ¨æŠ“å–ä¸­æ–­ {t_start}: {e}")
                raise e
            
    return {"status": "success", "chunks_processed": total_chunks}