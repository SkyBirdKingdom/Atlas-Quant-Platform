import logging
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.orm import Session
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from ..models import Trade, FetchState

logger = logging.getLogger("NordPoolFetcher")

# é…ç½®
AUTO_AREAS = ["SE1", "SE2", "SE3", "SE4"]
API_URL = "https://data-api.nordpoolgroup.com/api/v2/Intraday/Trades/ByDeliveryStart"

# --- 1. ç½‘ç»œå±‚ï¼šå¸¦è‡ªåŠ¨é‡è¯•çš„ API è¯·æ±‚ ---

# === é…ç½®åŒº ===
# é¦–æ¬¡è¿è¡Œæ—¶ï¼Œå¦‚æœæ²¡æœ‰å†å²è®°å½•ï¼Œä»è¿™ä¸ªæ—¶é—´ç‚¹å¼€å§‹æŠ“å–
# æ³¨æ„ï¼šæ ¼å¼å¿…é¡»æ˜¯ ISO 8601
INITIAL_START_DATE = "2025-01-01T00:00:00Z"

def get_token():
    token_url = "https://sts.nordpoolgroup.com/connect/token"
    headers = {"Content-Type": "application/x-www-form-urlencoded", "Authorization": "Basic Y2xpZW50X21hcmtldGRhdGFfYXBpOmNsaWVudF9tYXJrZXRkYXRhX2FwaQ=="}
    params = {"grant_type": "password", "scope": "marketdata_api", "username": "API_DATA_GreenVoltisSwedenAB", "password": "6meGT1)=WX85(aRm2b"}
    resp = requests.post(token_url, headers=headers, data=params, timeout=10)
    resp.raise_for_status()
    return resp.json().get("access_token")

# ä½¿ç”¨è£…é¥°å™¨å¤„ç†é‡è¯•ï¼š
# - æœ€å¤šé‡è¯• 5 æ¬¡
# - ç­‰å¾…æ—¶é—´æŒ‡æ•°å¢é•¿ (2s, 4s, 8s...)ï¼Œé˜²æ­¢æŠŠ NordPool å†²å®
# - åªåœ¨é‡åˆ° RequestException (ç½‘ç»œé”™, 500, 502) æ—¶é‡è¯•
@retry(
    stop=stop_after_attempt(5), 
    wait=wait_exponential(multiplier=1, min=2, max=60),
    retry=retry_if_exception_type(requests.RequestException)
)
def fetch_api_chunk(token, area, start_str, end_str):
    params = {"deliveryStartFrom": start_str, "deliveryStartTo": end_str, "areas": area}
    headers = {"accept": "application/json", "authorization": f"Bearer {token}"}
    
    resp = requests.get(API_URL, params=params, headers=headers, timeout=30)
    
    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœ Token è¿‡æœŸ (401)ï¼ŒæŠ›å‡ºé”™è¯¯è®©å¤–å±‚åˆ·æ–° Token
    if resp.status_code == 401:
        raise PermissionError("Token expired")
    
    resp.raise_for_status() # é 200 æŠ›å‡ºå¼‚å¸¸ï¼Œè§¦å‘é‡è¯•
    return resp.json()

# --- 2. æ•°æ®å¤„ç†ä¸å­˜å‚¨ ---

def flatten_and_parse(raw_data, area):
    rows = []
    volume_unit = raw_data.get('volumeUnit')
    
    for contract in raw_data.get('contracts', []) or []:
        base = {
            'contractId': contract.get('contractId'),
            'contractName': contract.get('contractName'),
            'deliveryStart': contract.get('deliveryStart'),
            'deliveryEnd': contract.get('deliveryEnd'),
        }
        for trade in contract.get('trades', []) or []:
            # è¿‡æ»¤ï¼šåªä¿ç•™ belongs to area çš„æ•°æ®
            # æˆ–è€…æ˜¯ legs é‡ŒåŒ…å«è¯¥ area
            legs = trade.get('legs') or []
            target_area_found = False
            
            # å¦‚æœæ²¡æœ‰ legs (å•è¾¹äº¤æ˜“?) æˆ–è€… legs é‡Œæœ‰å½“å‰åŒºåŸŸ
            if not legs:
                target_area_found = True # å‡è®¾ API ç­›é€‰è¿‡äº†
            else:
                for leg in legs:
                    if leg.get('deliveryArea') == area:
                        target_area_found = True
                        break
            
            if target_area_found:
                rows.append({
                    **base,
                    'tradeId': trade.get('tradeId'),
                    'tradeTime': trade.get('tradeTime'),
                    'price': trade.get('price'),
                    'volume': trade.get('volume'),
                    'deliveryArea': area 
                })
    return rows

def save_chunk_to_db(db: Session, data_list: list):
    if not data_list: return
    
    df = pd.DataFrame(data_list)
    for col in ['deliveryStart', 'deliveryEnd', 'tradeTime']:
        df[col] = pd.to_datetime(df[col], format='mixed')

    df['duration_minutes'] = (df['deliveryEnd'] - df['deliveryStart']).dt.total_seconds() / 60
    conditions = [(abs(df['duration_minutes']-60)<1), (abs(df['duration_minutes']-15)<1)]
    df['contract_type'] = np.select(conditions, ['PH', 'QH'], default='Other')

    records = df.to_dict(orient='records')
    
    # æ˜ å°„å­—æ®µå
    db_records = []
    for r in records:
        db_records.append({
            "trade_id": r['tradeId'],
            "contract_id": r['contractId'],
            "contract_name": r['contractName'],
            "price": r['price'],
            "volume": r['volume'],
            "delivery_area": r['deliveryArea'],
            "delivery_start": r['deliveryStart'],
            "delivery_end": r['deliveryEnd'],
            "trade_time": r['tradeTime'],
            "duration_minutes": r['duration_minutes'],
            "contract_type": r['contract_type']
        })

    stmt = insert(Trade).values(db_records)
    stmt = stmt.on_conflict_do_nothing(index_elements=['trade_id'])
    db.execute(stmt)

# --- 3. çŠ¶æ€ç®¡ç† ---

def update_fetch_state(db: Session, area: str, last_time=None, status="running", error=None):
    state = db.query(FetchState).filter(FetchState.area == area).first()
    if not state:
        state = FetchState(area=area)
        db.add(state)
    
    if last_time:
        state.last_fetched_time = last_time
    
    state.status = status
    state.updated_at = datetime.utcnow()
    if error:
        state.last_error = str(error)[:500] # æˆªæ–­é”™è¯¯ä¿¡æ¯é˜²æ­¢å¤ªé•¿
    else:
        state.last_error = None # æ¸…é™¤é”™è¯¯
        
    db.commit()

# --- 4. ä¸»åŒæ­¥é€»è¾‘ (å¢å¼ºç‰ˆ) ---

def sync_area_logic(db: Session, area: str):
    """
    ä¿®æ­£åçš„åŒæ­¥é€»è¾‘ï¼š
    1. å†å²æ•°æ®ï¼šæ¨è¿› Checkpointã€‚
    2. æ´»è·ƒæ•°æ®ï¼šæ¯æ¬¡å¼ºåˆ¶é‡åˆ·ï¼Œä¸æ¨è¿› Checkpointã€‚
    """
    # è·å–æ•°æ®åº“é‡Œçš„è¿›åº¦ï¼ˆè¿™æ˜¯â€œå·²å½’æ¡£â€çš„æ—¶é—´çº¿ï¼‰
    state = db.query(FetchState).filter(FetchState.area == area).first()
    
    if state and state.last_fetched_time:
        # åœºæ™¯ A: ä¸æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œæ¥ç€ä¸Šæ¬¡çš„è¿›åº¦è·‘
        archived_time = state.last_fetched_time
    else:
        # åœºæ™¯ B: ç¬¬ä¸€æ¬¡è¿è¡Œ (å†·å¯åŠ¨)ï¼Œä½¿ç”¨é…ç½®çš„åˆå§‹æ—¶é—´
        # å»æ‰ 'Z' å› ä¸º datetime.fromisoformat åœ¨æŸäº› Python ç‰ˆæœ¬å¯¹ Z æ”¯æŒä¸å¥½ï¼Œæˆ–è€…ç»Ÿä¸€ç”¨ replaceå¤„ç†
        archived_time = datetime.fromisoformat(INITIAL_START_DATE.replace('Z', ''))
        logger.info(f"[{area}] é¦–æ¬¡åˆå§‹åŒ–ï¼Œä»é…ç½®æ—¶é—´å¼€å§‹: {archived_time}")
    
    # å®šä¹‰â€œç°åœ¨â€å’Œâ€œæœªæ¥è¾¹ç•Œâ€
    now = datetime.utcnow()
    # æ´»è·ƒçª—å£ï¼šNord Pool æ—¥å†…é€šå¸¸æœ€å¤šå¼€æ”¾åˆ°æ˜å¤© (çº¦ +36~48 å°æ—¶)
    future_limit = now + timedelta(hours=48)
    
    # === ç¬¬ä¸€é˜¶æ®µï¼šè¿½èµ¶å†å² (Backfill) ===
    # è¿™é‡Œçš„ç›®æ ‡æ˜¯æŠŠ Checkpoint æ¨è¿›åˆ° "Now"
    # æˆ‘ä»¬è®¤ä¸ºï¼šDeliveryTime < Now çš„æ•°æ®ï¼ŒåŸºæœ¬å·²ç»ç¨³å®šï¼ˆè™½ç„¶ç†è®ºä¸Šäº¤å‰²å‰éƒ½èƒ½äº¤æ˜“ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾"è¿‡å»"çš„æ—¶é—´æ®µåªéœ€æ¨è¿›ï¼‰
    # æ›´ä¸¥è°¨çš„åšæ³•æ˜¯ï¼šCheckpoint åªèƒ½æ¨è¿›åˆ° "Now - äº¤ä»˜å‘¨æœŸæ—¶é•¿"ï¼Œæ¯”å¦‚ Now - 2å°æ—¶
    
    # è®¾å®šä¸€ä¸ªâ€œå®‰å…¨å½’æ¡£çº¿â€ï¼Œæ¯”å¦‚ 2 å°æ—¶å‰ã€‚åœ¨æ­¤ä¹‹å‰çš„äº¤ä»˜æ—¶æ®µï¼Œæˆ‘ä»¬è®¤ä¸ºæ•°æ®ä¸ä¼šå†å˜äº†ï¼Œå¯ä»¥æ›´æ–° Checkpointã€‚
    safe_archive_line = now - timedelta(hours=2)
    
    curr = archived_time
    token = get_token()
    
    # 1. å¾ªç¯æ¨è¿›å†å²è¿›åº¦
    while curr < safe_archive_line:
        # æ¯æ¬¡æ­¥è¿› 12 å°æ—¶
        chunk_end = min(curr + timedelta(hours=12), safe_archive_line)
        
        t_start = curr.strftime('%Y-%m-%dT%H:%M:%SZ')
        t_end = chunk_end.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        try:
            logger.info(f"[{area}] ğŸ“¥ è¡¥å½•å†å²: {t_start} -> {t_end}")
            
            # API è¯·æ±‚ä¸å…¥åº“
            try:
                raw = fetch_api_chunk(token, area, t_start, t_end)
            except PermissionError:
                token = get_token()
                raw = fetch_api_chunk(token, area, t_start, t_end)
            
            data = flatten_and_parse(raw, area)
            if data:
                save_chunk_to_db(db, data)
            
            # å…³é”®ï¼šå†å²æ•°æ®æŠ“å®Œä¸€æ®µï¼Œæ›´æ–°ä¸€æ¬¡æ•°æ®åº“ Checkpoint
            update_fetch_state(db, area, last_time=chunk_end, status="running")
            curr = chunk_end
            
        except Exception as e:
            logger.error(f"[{area}] å†å²è¡¥å½•å¤±è´¥: {e}")
            update_fetch_state(db, area, status="error", error=str(e))
            return # å†å²éƒ½æŒ‚äº†ï¼Œåé¢å°±åˆ«è·‘äº†
            
    # === ç¬¬äºŒé˜¶æ®µï¼šåˆ·æ–°æ´»è·ƒçª—å£ (Active Window) ===
    # ä» "å®‰å…¨å½’æ¡£çº¿" ä¸€ç›´æŠ“åˆ° "æœªæ¥è¾¹ç•Œ"
    # è¿™éƒ¨åˆ†æ•°æ®ç»å¯¹ã€ä¸èƒ½ã€‘æ›´æ–° Checkpointï¼Œå› ä¸ºä¸‹ä¸€å°æ—¶è¿˜è¦å†æ¥æŠ“ä¸€éæ–°æˆäº¤çš„
    
    active_start = curr # æ¥ç€ä¸Šé¢çš„è¿›åº¦
    logger.info(f"[{area}] ğŸ”„ åˆ·æ–°æ´»è·ƒçª—å£: {active_start.strftime('%Y-%m-%dT%H:%M')} -> {future_limit.strftime('%Y-%m-%dT%H:%M')}")
    
    while active_start < future_limit:
        chunk_end = min(active_start + timedelta(hours=12), future_limit)
        
        t_start = active_start.strftime('%Y-%m-%dT%H:%M:%SZ')
        t_end = chunk_end.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        try:
            # API è¯·æ±‚
            try:
                raw = fetch_api_chunk(token, area, t_start, t_end)
            except PermissionError:
                token = get_token()
                raw = fetch_api_chunk(token, area, t_start, t_end)
            
            data = flatten_and_parse(raw, area)
            
            # å…¥åº“ (åˆ©ç”¨æ•°æ®åº“çš„ ON CONFLICT DO NOTHING å»é‡)
            # è™½ç„¶æˆ‘ä»¬é‡å¤æŠ“å–äº†ï¼Œä½†æ•°æ®åº“é‡Œå·²æœ‰çš„ TradeID ä¼šè¢«å¿½ç•¥ï¼Œåªæœ‰æ–°äº§ç”Ÿçš„ TradeID ä¼šè¢«æ’å…¥
            if data:
                save_chunk_to_db(db, data)
                logger.info(f"[{area}] æ´»è·ƒçª—å£æ›´æ–°: æŠ“å– {len(data)} æ¡")
            
            # æ³¨æ„ï¼šè¿™é‡Œã€ä¸è°ƒç”¨ã€‘update_fetch_state æ¥æ›´æ–° last_time
            active_start = chunk_end
            
        except Exception as e:
            logger.error(f"[{area}] æ´»è·ƒçª—å£åˆ·æ–°å¤±è´¥: {e}")
            # æ´»è·ƒçª—å£å¶å°”å¤±è´¥ä¸å½±å“å¤§å±€ï¼Œè®°å½•é”™è¯¯å³å¯
            update_fetch_state(db, area, status="warning", error=f"Active window error: {e}")
            break

    # å…¨éƒ¨è·‘å®Œï¼ŒçŠ¶æ€æ ‡ä¸º OK
    update_fetch_state(db, area, status="ok")

def sync_all_areas(db: Session):
    """
    å…¥å£å‡½æ•°ï¼šéå†æ‰€æœ‰åŒºåŸŸ
    """
    logger.info("â° å¯åŠ¨å®šæ—¶åŒæ­¥...")
    for area in AUTO_AREAS:
        try:
            sync_area_logic(db, area)
        except Exception as e:
            logger.error(f"âŒ [{area}] ä»»åŠ¡ä¸­æ–­: {e}")
            # è¿™é‡Œ catch ä½ï¼Œä¿è¯ SE3 æŒ‚äº†ä¸å½±å“ SE4 ç»§ç»­è·‘
            continue
    logger.info("âœ… å®šæ—¶åŒæ­¥ç»“æŸ")